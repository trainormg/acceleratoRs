{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "output:\n",
    "  html_notebook: default\n",
    "  output: rticles::ctex\n",
    "  pdf_document: default\n",
    "author: \"Le Zhang, Data Scientist, Microsoft\"\n",
    "date: '`r Sys.Date()`'\n",
    "title: \"Employee Attrition Prediction with Sentiment Analysis\"\n",
    "documentclass: ctexart\n",
    "---\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "Voluntary employee attrition may negatively affect a company in various aspects, i.e., induce labor cost, lose morality of employees, leak IP/talents to competitors, etc. Identifying individual employee with inclination of leaving company is therefore pivotal to save the potential loss. Conventional practices rely on qualitative assessment on factors that may reflect the propensity of an employee to leave company. For example, studies found that staff churn is correlated with both demographic information as well as behavioral activities, satisfaction, etc. Data-driven techniques which are based on statistical learning methods exhibit more accurate prediction on employee attrition, as by nature they mathematically model the correlation between factors and attrition outcome and maximize the probability of predicting the correct group of people with a properly trained machine learning model.\n",
    "\n",
    "In the data-driven employee attrition prediction model, normally two types of data are taken into consideration. \n",
    "\n",
    "1. First type refers to the demographic and organizational information of an employee such as *age*, *gender*, *title*, etc. The characteristics of this group of data is that **within a certain interval, they don't change or solely increment deterministically over time**. For example, gender will never change for an individual, and other factors such as *years of service* increments every year. \n",
    "\n",
    "2. Second type of data is the dynamically involving information about an employee. Recent [studies](http://www.wsj.com/articles/how-do-employees-really-feel-about-their-companies-1444788408) report that *sentiment* is playing a critical role in employee attrition prediction. Classical measures of sentiment include *job satisfaction*, *environment satisfaction*, *relationship satisfaction*, etc. With the machine learning techniques, sentiment patterns can be exploited from daily activities such as text posts on social media for predicting churn inclination.\n",
    "\n",
    "## 2 Data-driven analytics for HR attrition prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "echo": "TRUE,",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "\n",
    "library(dplyr)\n",
    "library(magrittr)\n",
    "library(stringr)\n",
    "library(stringi)\n",
    "library(readr)\n",
    "\n",
    "# machine learning and advanced analytics\n",
    "\n",
    "library(DMwR)\n",
    "library(caret)\n",
    "library(caretEnsemble)\n",
    "library(pROC)\n",
    "\n",
    "# natural language processing\n",
    "\n",
    "library(msLanguageR)\n",
    "library(tm)\n",
    "library(jiebaR)\n",
    "\n",
    "# tools\n",
    "\n",
    "library(httr)\n",
    "library(XML)\n",
    "library(jsonlite)\n",
    "\n",
    "# data visualization\n",
    "\n",
    "library(scales)\n",
    "library(ggplot2)\n",
    "library(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global variables\n",
    "\n",
    "DATA1 <- \"../Data/DataSet1.csv\"\n",
    "DATA2 <- \"../Data/DataSet2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Demographic and organizational data\n",
    "\n",
    "The experiments will be conducted on a data set of employees. The data set is publicly available and can be found at [here](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/).\n",
    "\n",
    "#### 2.1.1 Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- read_csv(DATA1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains 1470 rows, each of which includes 34 variables of an employee. The column of \"Attrition\" is the label of employees about their employment status with the company. The other 33 variables are those which are considered relevant to the label variable. Both demographic data (e.g., *gender*, *age*, etc.), and sentiment data (e.g., *job satisfaction*, etc.) are included. \n",
    "\n",
    "#### 2.1.2 Visualization of data\n",
    "\n",
    "Initial exploratory analysis can be performed to understand the data set. For example,\n",
    "\n",
    "1. the proportion of employees with different job titles (or any other possible factor) for status of \"attrition\" and \"non-attrition\" may vary, and this can be plotted as follows. People titled \"Laboratory Technician\", \"Sales Executive\", and \"Research Scientist\" are among the top 3 groups that exhibit highest attrition rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(df, aes(JobRole, fill=Attrition)) +\n",
    "  geom_bar(aes(y=(..count..)/sum(..count..)), position=\"dodge\") +\n",
    "  scale_y_continuous(labels=percent) +\n",
    "  xlab(\"Job Role\") +\n",
    "  ylab(\"Percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. monthly income, job level, and service year may affect decision of leaving for employees in different departments. For example, junior staffs with lower pay will be more likely to leave compared to those who are paid higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(filter(df, (YearsAtCompany >= 2) & (YearsAtCompany <= 5) & (JobLevel < 3)),\n",
    "       aes(x=factor(JobRole), y=MonthlyIncome, color=factor(Attrition))) +\n",
    "  geom_boxplot() +\n",
    "  xlab(\"Department\") +\n",
    "  ylab(\"Monthly income\") +\n",
    "  scale_fill_discrete(guide=guide_legend(title=\"Attrition\")) +\n",
    "  theme_bw() +\n",
    "  theme(text=element_text(size=13), legend.position=\"top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Promotion is a commonly adopted HR strategy for employee retention. It can be observed in the following plot that for a certain department, e.g., Research & Development, employees with higher job level is more likely to leave if there are years since their last promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(filter(df, as.character(Attrition) == \"Yes\"), aes(x=YearsSinceLastPromotion)) +\n",
    "  geom_histogram(binwidth=0.5) +\n",
    "  aes(y=..density..) +\n",
    "  xlab(\"Years since last promotion.\") +\n",
    "  ylab(\"Density\") +\n",
    "  # scale_fill_discrete(guide=guide_legend(title=\"Attrition\")) +\n",
    "  facet_grid(Department ~ JobLevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Data pre-processing\n",
    "\n",
    "To perform further advanced analysis on the data set, initial pre-processing is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictors that has no variation.\n",
    "\n",
    "pred_no_var <- names(df[, nearZeroVar(df)]) %T>% print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the zero variation predictor columns.\n",
    "\n",
    "df %<>% select(-one_of(pred_no_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer types of predictors which are nominal are converted to categorical type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert certain integer variable to factor variable.\n",
    "\n",
    "int_2_ftr_vars <- c(\"Education\", \"EnvironmentSatisfaction\", \"JobInvolvement\", \"JobLevel\", \"JobSatisfaction\", \"NumCompaniesWorked\", \"PerformanceRating\", \"RelationshipSatisfaction\", \"StockOptionLevel\")\n",
    "\n",
    "df[, int_2_ftr_vars] <- lapply((df[, int_2_ftr_vars]), as.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables of character type are converted to categorical type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df %<>% mutate_if(is.character, as.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Problem formalization\n",
    "\n",
    "After the data is well prepared, a model can be constructed for attrition prediction. Normally employee attrition prediction is categorized as a binary classification problem, i.e., to predict *whether or not an employee will leave*.\n",
    "\n",
    "In this study case, the label for prediction is employee status, named as `Attrition` in the data set, which has two levels, `Yes` and `No`, indicating that the employee has left or stayed. \n",
    "\n",
    "Check the label column to make sure it is a factor type, as the model to be built is a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is.factor(df$Attrition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5 Feature selection\n",
    "\n",
    "It is possible that not all variables are correlated with the label, feature selection is therefore performed to filter out the most relevant ones. \n",
    "\n",
    "As the data set is a blend of both numerical and discrete variables, certain correlation analysis (e.g., Pearson correlation) is not applicable. One alternative is to train a model and then rank the variable importance so as to select the most salient ones.\n",
    "\n",
    "The following shows how to achieve variable importance ranking with a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "echo": "TRUE,",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# set up the training control.\n",
    "\n",
    "control <- trainControl(method=\"repeatedcv\", number=3, repeats=1)\n",
    "\n",
    "# train the model\n",
    "\n",
    "model <- train(dplyr::select(df, -Attrition), \n",
    "               df$Attrition,\n",
    "               data=df, \n",
    "               method=\"rf\", \n",
    "               preProcess=\"scale\", \n",
    "               trControl=control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate variable importance\n",
    "\n",
    "imp <- varImp(model, scale=FALSE)\n",
    "\n",
    "# plot\n",
    "\n",
    "plot(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top-ranking variables.\n",
    "\n",
    "imp_list <- rownames(imp$importance)[order(imp$importance$Overall, decreasing=TRUE)]\n",
    "\n",
    "# drop the low ranking variables. Here the last 3 variables are dropped. \n",
    "\n",
    "top_var <- \n",
    "  imp_list[1:(ncol(df) - 3)] %>%\n",
    "  as.character() \n",
    "\n",
    "top_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top ranking variables \n",
    "\n",
    "df %<>% select(., one_of(c(top_var, \"Attrition\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6 Resampling\n",
    "\n",
    "A prediction model can be then created for predictive analysis. The whole data is split into training and testing sets. The former is used for model creation while the latter for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index <- \n",
    "  createDataPartition(df$Attrition,\n",
    "                      times=1,\n",
    "                      p=.7) %>%\n",
    "  unlist()\n",
    "\n",
    "df_train <- df[train_index, ]\n",
    "df_test <- df[-train_index, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing worthnoting is that the training set is not balanced, which may deteriorate the performance in training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(df_train$Attrition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active employees (864) are more than terminated employees (166). There are several ways to deal with data imbalance issue:\n",
    "\n",
    "1. Resampling the data - either upsampling the minority class or downsampling the majority class.\n",
    "2. Use cost sensitive learning method.\n",
    "\n",
    "In this case the first method is used. SMOTE is a commonly adopted method for synthetically upsampling minority class in an imbalanced data set. Package `DMwR` provides methods that apply SMOTE methods on training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# note DMwR::SMOTE does not handle well with tbl_df. Need to convert to data frame.\n",
    "\n",
    "df_train %<>% as.data.frame()\n",
    "\n",
    "df_train <- SMOTE(Attrition ~ .,\n",
    "                  df_train,\n",
    "                  perc.over=300,\n",
    "                  perc.under=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(df_train$Attrition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.7 Model building\n",
    "\n",
    "After balancing the training set, a model can be created for prediction. For comparison purpose, different individual models, as well as ensemble of them, are trained on the data set. `caret` and `caretEnsemble` packages are used for training models.\n",
    "\n",
    "1. Individual models.\n",
    "Three algorithms, support vector machine with radial basis function kernel, random forest, and extreme gradient boosting (xgboost), are used for model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "echo": "TRUE,",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# initialize training control. \n",
    "tc <- trainControl(method=\"boot\", \n",
    "                   number=3, \n",
    "                   repeats=3, \n",
    "                   search=\"grid\",\n",
    "                   classProbs=TRUE,\n",
    "                   savePredictions=\"final\",\n",
    "                   summaryFunction=twoClassSummary)\n",
    "\n",
    "# SVM model.\n",
    "\n",
    "time_svm <- system.time(\n",
    "  model_svm <- train(Attrition ~ .,\n",
    "                     df_train,\n",
    "                     method=\"svmRadial\",\n",
    "                     trainControl=tc)\n",
    ")\n",
    "\n",
    "# random forest model\n",
    "\n",
    "time_rf <- system.time(\n",
    "  model_rf <- train(Attrition ~ .,\n",
    "                     df_train,\n",
    "                     method=\"rf\",\n",
    "                     trainControl=tc)\n",
    ")\n",
    "\n",
    "# xgboost model.\n",
    "\n",
    "time_xgb <- system.time(\n",
    "  model_xgb <- train(Attrition ~ .,\n",
    "                     df_train,\n",
    "                     method=\"xgbLinear\",\n",
    "                     trainControl=tc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ensemble of models.\n",
    "Model ensemble is also created for comparative studies on performance. Here a stacking ensemble is demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "echo": "TRUE,",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# ensemble of the three models.\n",
    "\n",
    "time_ensemble <- system.time(\n",
    "  model_list <- caretList(Attrition ~ ., \n",
    "                          data=df_train,\n",
    "                          trControl=tc,\n",
    "                          methodList=c(\"svmRadial\", \"rf\", \"xgbLinear\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack of models. Use glm for meta model.\n",
    "\n",
    "model_stack <- caretStack(\n",
    "  model_list,\n",
    "  metric=\"ROC\",\n",
    "  method=\"glm\",\n",
    "  trControl=trainControl(\n",
    "    method=\"boot\",\n",
    "    number=10,\n",
    "    savePredictions=\"final\",\n",
    "    classProbs=TRUE,\n",
    "    summaryFunction=twoClassSummary\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.8 Model validation\n",
    "\n",
    "The trained models are applied on testing data sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models <- list(model_svm, model_rf, model_xgb, model_stack)\n",
    "\n",
    "predictions <-lapply(models, \n",
    "                     predict, \n",
    "                     newdata=select(df_test, -Attrition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix evaluation results.\n",
    "\n",
    "cm_metrics <- lapply(predictions,\n",
    "                     confusionMatrix, \n",
    "                     reference=df_test$Attrition, \n",
    "                     positive=\"Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be then comparatively studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "\n",
    "acc_metrics <- \n",
    "  lapply(cm_metrics, `[[`, \"overall\") %>%\n",
    "  lapply(`[`, 1) %>%\n",
    "  unlist()\n",
    "\n",
    "# recall\n",
    "\n",
    "rec_metrics <- \n",
    "  lapply(cm_metrics, `[[`, \"byClass\") %>%\n",
    "  lapply(`[`, 1) %>%\n",
    "  unlist()\n",
    "  \n",
    "# precision\n",
    "\n",
    "pre_metrics <- \n",
    "  lapply(cm_metrics, `[[`, \"byClass\") %>%\n",
    "  lapply(`[`, 3) %>%\n",
    "  unlist()\n",
    "\n",
    "algo_list <- c(\"SVM RBF\", \"Random Forest\", \"Xgboost\", \"Stacking\")\n",
    "time_consumption <- c(time_svm[3], time_rf[3], time_xgb[3], time_ensemble[3])\n",
    "\n",
    "df_comp <- \n",
    "  data.frame(Models=algo_list, \n",
    "             Accuracy=acc_metrics, \n",
    "             Recall=rec_metrics, \n",
    "             Precision=pre_metrics,\n",
    "             Time=time_consumption) %T>%\n",
    "             {head(.) %>% print()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sentiment analysis\n",
    "\n",
    "#### 2.2.1 Rating score.\n",
    "\n",
    "Besides the demographic and organizational data, sentiment data may also reflect intention of leave. For example, ratings in employee survey such as job satisfaction may reflect the feelings of employees about company (see plot below). As can be seen in the three plots, employees that have left the company expressed more negatively (i.e., proportion of rating 1 is more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(df, aes(JobSatisfaction, fill=Attrition)) +\n",
    "  geom_bar(aes(y=(..count..)/sum(..count..)), position=\"dodge\") +\n",
    "  scale_y_continuous(labels=percent) +\n",
    "  xlab(\"Job Satisfaction\") +\n",
    "  ylab(\"Percentage\")\n",
    "\n",
    "ggplot(df, aes(RelationshipSatisfaction, fill=Attrition)) +\n",
    "  geom_bar(aes(y=(..count..)/sum(..count..)), position=\"dodge\") +\n",
    "  scale_y_continuous(labels=percent) +\n",
    "  xlab(\"Relationship Satisfaction\") +\n",
    "  ylab(\"Percentage\")\n",
    "\n",
    "ggplot(df, aes(EnvironmentSatisfaction, fill=Attrition)) +\n",
    "  geom_bar(aes(y=(..count..)/sum(..count..)), position=\"dodge\") +\n",
    "  scale_y_continuous(labels=percent) +\n",
    "  xlab(\"Environment Satisfaction\") +\n",
    "  ylab(\"Percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be observed from the data set that within the group of churned employees, population of lower satisfaction score is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(df, aes(x=factor(Attrition), fill=factor(JobSatisfaction))) +\n",
    "  geom_bar(width=0.5, position=\"fill\") +\n",
    "  coord_flip() +\n",
    "  xlab(\"Attrition\") +\n",
    "  ylab(\"Proportion\") +\n",
    "  scale_fill_discrete(guide=guide_legend(title=\"Score of\\n relationship satisfaction\")) \n",
    "\n",
    "ggplot(df, aes(x=factor(Attrition), fill=factor(RelationshipSatisfaction))) +\n",
    "  geom_bar(width=0.5, position=\"fill\") +\n",
    "  coord_flip() +\n",
    "  xlab(\"Attrition\") +\n",
    "  ylab(\"Proportion\") +\n",
    "  scale_fill_discrete(guide=guide_legend(title=\"Score of\\n relationship satisfaction\")) \n",
    "\n",
    "ggplot(df, aes(x=factor(Attrition), fill=factor(EnvironmentSatisfaction))) +\n",
    "  geom_bar(width=0.5, position=\"fill\") +\n",
    "  coord_flip() +\n",
    "  xlab(\"Attrition\") +\n",
    "  ylab(\"Proportion\") +\n",
    "  scale_fill_discrete(guide=guide_legend(title=\"Score of\\n environment satisfaction\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Review comments\n",
    "\n",
    "As the proliferation of social media, employees' posts onto social media website may be collected for churn analysis. The hypothesis is that the frequency pattern of terms used by employees that leave is statistically different from that of those that stay. As the original text of social media post and chat may be noisy and random. Pre-processing work such as removal of stop words, sparse terms, and punctuations is required. \n",
    "\n",
    "To illustrate, a data set containing review comments of 500 employees about their company is used. The review comments were obtained from [Glassdoor](www.glassdoor.com), which were posted by employees that are currently with and have left the company. Note since the post are anonymous so it may not accurately reflect the true feeling of an employee towards the employer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data.\n",
    "\n",
    "df <-\n",
    "  read_csv(DATA2) %>%\n",
    "  mutate(Feedback=as.character(Feedback))\n",
    "\n",
    "head(df$Feedback, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text can be pre-processed with `tm` package. Normally to process text for quantitative analysis, the original non-structural data in text format needs to be transformed into vector. \n",
    "\n",
    "For the convenient of text-to-vector transformation, the original review comment data is wrapped into a corpus format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus based upon the text data.\n",
    "\n",
    "corp_text <- Corpus(VectorSource(df$Feedback))\n",
    "\n",
    "corp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tm_map` function in `tm` package helps perform translation on the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transformation functions can be checked with \n",
    "\n",
    "getTransformations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptions for each transformation is summarised as follows.\n",
    "\n",
    "|Function name|Description|\n",
    "|--------------------|---------------------------------------------------|\n",
    "|`removeNumbers`|Remove numbers from a text document.|\n",
    "|`removePunctuation`|Remove punctuation marks from a text document.|\n",
    "|`removeWords`|Remove words from a text document.|\n",
    "|`stemDocument`|Stem words in a text document using Porter's stemming algorithm.|\n",
    "|`stripWhitespace`|Strip extra whitespace from a text document. Multiple whitespace characters are collapsed to a single blank.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation on the corpus.\n",
    "\n",
    "corp_text %<>%\n",
    "  tm_map(removeNumbers) %>%\n",
    "  tm_map(content_transformer(tolower)) %>%\n",
    "  tm_map(removeWords, stopwords(\"english\")) %>%\n",
    "  tm_map(removePunctuation) %>%\n",
    "  tm_map(stripWhitespace) \n",
    "\n",
    "corp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The produced corpus can be converted to a term frequency matrix that contains the vector of the terms extracted from the corpus. There are two types of weighting methods supported in `tm` package, i.e., `weightTf` and `weightTfIdf`. The former calculates the term frequency as quantitative representation of corpus terms, while the latter calculates TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform corpus to document term frequency.\n",
    "\n",
    "dtm_txt_tf <- \n",
    "  DocumentTermMatrix(corp_text, control=list(wordLengths=c(1, Inf), weighting=weightTf)) \n",
    "\n",
    "inspect(dtm_txt_tf[1:10, 1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the original term frequency matrix is very sparse. `removeSparseTerms` can be used for removing sparse terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_txt <-\n",
    "  removeSparseTerms(dtm_txt_tf, 0.99) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finalized matrix can be converted to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt <- \n",
    "  inspect(dtm_txt) %>%\n",
    "  as.data.frame()\n",
    "\n",
    "head(df_txt, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Multi-lingual sentiment analysis\n",
    "It is common to see employees in multinational corporations using different languages. To this end, analysis on multi-lingual text is necessary.\n",
    "\n",
    "There are basically two methods of doing it.\n",
    "\n",
    "1. Translate text in various languages into one target language and performance the analysis. This can be done directly with translation APIs provided by Microsoft or Google. The example below shows how to use Microsoft Cognitive Services API for text translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "eval": "FALSE",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the API keys.\n",
    "\n",
    "source(\"path_to_your_confidential_information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "eval": "FALSE",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "text <- \"我非常喜欢现在的工作\"\n",
    "translated_text1 <- cognitiveTranslation(text, lanFrom=\"zh-CHS\", lanTo=\"en\", apiKey=\"your_api_key\")\n",
    "\n",
    "translated_text1\n",
    "# [1] \"I really like the current job.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "eval": "FALSE",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "text <- \"Big Data\"\n",
    "translated_text2 <- cognitiveTranslation(text, lanFrom=\"en\", lanTo=\"zh-CHS\", apiKey=\"a valid key\")\n",
    "\n",
    "translated_text2\n",
    "# [1] \"大数据\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Second approach is relying on language specific tokenizer. For instance, `jiebaR` provides methods to process Chinese together with English, which can be then converted to vector that is comfortable with `tm` functions.\n",
    "\n",
    "The following sample set shows how this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "eval": "FALSE",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample text data.\n",
    "\n",
    "df_text <- data.frame(\n",
    "  text = c(\n",
    "    \"工作好辛苦\",\n",
    "    \"每天都只有辛苦的工作\",\n",
    "    \"今天非常开心！\",\n",
    "    \"It is a great honor to work in Microsoft.\",\n",
    "    \"工作压力巨大。\",\n",
    "    \"吃了喜欢的拉面，开心！\",\n",
    "    \"weekend要工作，难过……\",\n",
    "    \"今天放假！\",\n",
    "    \"不想工作。\"\n",
    "  ),\n",
    "  mood = c( # P is positive and N is negative.\n",
    "    \"N\",\n",
    "    \"N\",\n",
    "    \"P\",\n",
    "    \"N\",\n",
    "    \"P\",\n",
    "    \"P\",\n",
    "    \"N\",\n",
    "    \"P\",\n",
    "    \"N\"\n",
    "  ),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "print(df_text$text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "eval": "FALSE",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# firstly make it a corpus.\n",
    "\n",
    "corp_text <- \n",
    "  Corpus(VectorSource(df_text$text)) %>%\n",
    "  tm_map(removePunctuation) %>%\n",
    "  tm_map(removeWords, stopwords(\"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a tokenier. Specify the Chinese stop words dictionary.\n",
    "\n",
    "cutter2 <- worker(stop_word = \"../Data/DataSet3.csv\", bylines = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note as `jiebaR` does not provide methods for term frequency transformation from corpus, `tm` is used for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "eval": "FALSE",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# customize a tokenizer function that can be embedded into DocumentTermMatrix function.\n",
    "\n",
    "jieba_tokenizer <- function(d) {\n",
    "  unlist(segment(d[[1]], cutter2))\n",
    "}\n",
    "\n",
    "dtm_text <- \n",
    "  DocumentTermMatrix(corp_text, \n",
    "                     control = list(wordLengths = c(1, Inf),\n",
    "                                    weighting = weightTf, \n",
    "                                    tokenize = jieba_tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "eval": "FALSE",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# produce a data frame of document term frequency.\n",
    "df_dtf <- \n",
    "  as.data.frame(inspect(dtm_text)) %>%\n",
    "  cbind(mood = df_text$mood) %>%\n",
    "  arrange(mood) %T>%\n",
    "  {head(.) %>% print()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Sentiment analysis on review comments\n",
    "\n",
    "Sentiment analysis on text data by machine learning techniques is discussed in details [Pang's paper](http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf). Basically, the given text data that is labelled with different sentiment is firstly tokenized into segmented terms. Term frequencies, or combined with inverse document term frequencies, are then generated as feature vectors for the text. \n",
    "\n",
    "Sometimes multi-gram and part-of-speech tag are also included in the feature vectors. [Pang's studies](http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf) conclude that the performance of unigram features excel over other hybrid methods in terms of model accuracy.\n",
    "\n",
    "The problem can be defined as a classification problem - given the training data where each piece of text is labelled with employee status, a model can be obtained, which in turn can predict the inclination of employees to leave company. \n",
    "\n",
    "The model training part is similar to other classification problem. The previously processed data `df_txt` is used for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form the data set\n",
    "\n",
    "df_txt %<>% cbind(Attrition=df$Attrition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set into training and testing set.\n",
    "\n",
    "train_index <- \n",
    "  createDataPartition(df_txt$Attrition,\n",
    "                      times=1,\n",
    "                      p=.7) %>%\n",
    "  unlist()\n",
    "\n",
    "df_txt_train <- df_txt[train_index, ]\n",
    "df_txt_test <- df_txt[-train_index, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with RBF kernel is used as an illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "echo": "TRUE,",
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "# model building\n",
    "\n",
    "model_svm <- train(Attrition ~ .,\n",
    "                   df_txt_train,\n",
    "                   method=\"svmRadial\",\n",
    "                   trainControl=tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "\n",
    "prediction <- predict(model_svm, newdata=select(df_txt_test, -Attrition))\n",
    "\n",
    "confusionMatrix(prediction,\n",
    "                reference=df_txt_test$Attrition,\n",
    "                positive=\"Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis on text data can also be done with Text Analytics API of Microsoft Cognitive Services. Package `msLanguageR` wraps functions that call the API for generating sentiment scores.\n",
    "\n",
    "`msLanguageR` can be installed from GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install devtools\n",
    "if(!require(\"devtools\")) install.packages(\"devtools\")\n",
    "devtools::install_github(\"yueguoguo/Azure-R-Interface/utils/msLanguageR\")\n",
    "library(msLanguageR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "eval": "FALSE",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "senti_score <- cognitiveSentiAnalysis(text=df[-train_index, ]$Feedback, apiKey=\"your_api_key\")\n",
    "\n",
    "df_senti <- mutate(senti_score$documents, Attrition=ifelse(score < 0.5, \"Yes\", \"No\"))\n",
    "                   \n",
    "confusionMatrix(df_senti$Attrition,\n",
    "                reference=df[-train_index, ]$Attrition,\n",
    "                positive=\"Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this method is not applicable to languages other than the supported ones. For instance, for analyzing Chinese, text data needs to be translated into English firstly. This can be done with Bing Translation API, which is available in `msLanguageR` package as `cognitiveTranslation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "eval": "FALSE",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "text_translated <- lapply(df_text$text, cognitiveTranslation,\n",
    "                          lanFrom=\"zh-CHS\",\n",
    "                          lanTo=\"en\",\n",
    "                          apiKey=\"your_api_key\")\n",
    "\n",
    "text_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This document introduces a data-driven approach for employee attrition prediction with sentiment analysis. Techniques of data analysis, model building, and natural language processing are demonstrated on sample data. The walk through may help corporate HR department or relevant organization to plan in advance for saving any potential loss in recruiting and training."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
